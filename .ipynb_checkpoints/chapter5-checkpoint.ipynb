{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b154435d-bc10-472f-aa0a-a778d868d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2model.gpt import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da9c5d2-1b5b-437c-bca4-e764565cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 256,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e43d5d45-4d00-4f94-a44c-d5fde08d11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cfg = GPT_CONFIG_124M\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # model= kwargs.get('model')\n",
    "    # idx = kwargs.get('idx')\n",
    "    # max_new_tokens = kwargs.get('max_new_tokens')\n",
    "    # context_size = kwargs.get('context_size')\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280b808d-4406-473d-af87-28ed191918af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_ids(text, tokenizer):\n",
    "    ids = tokenizer.encode(text, allowed_special={'<endoftext>'})\n",
    "    encoded_batch = torch.tensor(ids).unsqueeze(0)\n",
    "    return encoded_batch\n",
    "\n",
    "def ids_to_text(ids, tokenizer):\n",
    "    flat = torch.tensor(ids).squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c84d5cd-9ac4-44dc-8afd-fadc4333a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am khan Meet Mal crowdfunding pardon Circle outsider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55275/3930258963.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat = torch.tensor(ids).squeeze(0)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = 'I am khan'\n",
    "\n",
    "generated_ids = generate_text_simple(model=model, \n",
    "                     idx = text_to_ids(text, tokenizer), \n",
    "                     max_new_tokens=6, \n",
    "                    context_size=cfg['context_length'])\n",
    "print(ids_to_text(generated_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "045eac2d-89b8-4542-8576-9945e0850304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'every effort moves'\n",
    "text2 = 'I really like'\n",
    "inputs = torch.tensor([tokenizer.encode(text1), tokenizer.encode(text2)])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a60356-2424-41fb-bc2a-c24d92c727f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16833,  3626,  6100],\n",
       "        [27485,   588, 11311]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'every effort moves'\n",
    "text2 = 'really like chocolate'\n",
    "targets = torch.tensor([tokenizer.encode(text1), tokenizer.encode(text2)])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223a92b6-a48f-4c40-b21e-bbae1c5d6491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e33234-df7d-4d43-a4b2-612b72d02ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[17979],\n",
       "         [ 8285],\n",
       "         [10625]],\n",
       "\n",
       "        [[44973],\n",
       "         [  853],\n",
       "         [25086]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb8f4ea-5b36-4321-b198-e224599287a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17979,  8285, 10625])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78d19a4-84ba-4a81-8e66-9ac9ba5d791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch1: every effort moves\n",
      "outputs batch 1:  shaderegon dreams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55275/3930258963.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat = torch.tensor(ids).squeeze(0)\n"
     ]
    }
   ],
   "source": [
    "print(f'Targets batch1: {ids_to_text(targets[0], tokenizer)}')\n",
    "print(f'outputs batch 1: {ids_to_text(token_ids[0].flatten(), tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb12227-d427-4cc5-83a8-844849b9f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6607e-05, 3.7716e-05, 1.0087e-05])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "target_probas_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f32f28e-a7f6-4ba1-b929-21ee7c68d469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4989e-05, 1.0154e-05, 3.3944e-05])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "target_probas_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e61190-79de-4d61-9a31-624850f8a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.7952)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_1)))\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc8b387e-e86d-437c-b1e7-56492bb02149",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten()\n",
    "# print(targets_flat)\n",
    "# logits_flat[,targets_flat]\n",
    "# print(logits_flat.shape)\n",
    "# logg = torch.log(,targets_flat])\n",
    "# logg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ade847c-ad33-4570-911f-685e99f027d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8468)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b9d6458-1b8b-40c6-9eb6-21e57b9627a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity measure for loss is : 51370.23046875\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(f'Perplexity measure for loss is : {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a5443c6-2ff1-466d-adae-25c11f302335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "981f27cb-18b2-4f56-b722-f9070b36ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words: 20479\n",
      "The total tokens 5145\n"
     ]
    }
   ],
   "source": [
    "print(f'The total words: {len(text_data)}')\n",
    "print(f'The total tokens {len(tokenizer.encode(text_data))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e73c490b-0e49-4975-8c16-993607d60f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size : 18431 and Validation data size 2048\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio*len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "print(f'Train data size : {len(train_data)} and Validation data size {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6af4f7f8-84ec-46f4-bbe0-48b388f4d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2model.data import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9c620ac-bf42-41cf-8d75-b45a361ec77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "                    train_data,\n",
    "                    batch_size=2,\n",
    "                    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0\n",
    "                    )\n",
    "val_loader = create_dataloader_v1(\n",
    "                    val_data,\n",
    "                    batch_size=2,\n",
    "                    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    num_workers=0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01a11c4a-4f03-4652-a755-1fdd7008b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Validation Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print('Train Loader: ')\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "      \n",
    "print('Validation Loader: ')\n",
    "for x,y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1886ed1b-25b9-4651-963a-f8b7eb4cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the batch loss\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a3564c-d963-4294-bd27-8066d93b3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader)==0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    \n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i< num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device)\n",
    "            total_loss +=loss.item()\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed85ede1-635d-4efe-bf55-554ac435478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (final_norm): LayerNorm()\n",
       "  (final_linear_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0744476-8630-41b7-af6f-bee65624503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98483975728353\n",
      "Validation loss: 11.023316383361816\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model ,device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "    print('Training loss:', train_loss)    \n",
    "    print('Validation loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb77cacf-9e81-472a-8fde-64978bd1f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        \n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57f3cc00-10d5-4437-964e-f39dfb7e6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model, idx=encoded, context_size=context_size\n",
    "        )\n",
    "        decoded_text = ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace('\\n', ' '))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f798c6-ec12-4a2f-94bf-25822167ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe5df4ae-47bf-445d-bbf6-4fc0d20c40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                      optimizer, device, num_epochs,\n",
    "                      eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen +=input_batch.numel()\n",
    "            global_step+=1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(f\"Ep {epoch+1} (step {global_step:06d}): \"\n",
    "                     f\"Train loss {train_loss:.3f}, \"\n",
    "                     f\"Val_loss {val_loss: .3f}\")\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7a34853-2568-405d-872b-00e2a8eeecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (step 000000): Train loss 9.731, Val_loss  9.915\n",
      "Ep 1 (step 000005): Train loss 7.968, Val_loss  8.395\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_text_simple() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     16\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 19\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 30\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m             track_tokens_seen\u001b[38;5;241m.\u001b[39mappend(tokens_seen)\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal_loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mgenerate_and_print_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_context\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, val_losses, track_tokens_seen\n",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m, in \u001b[0;36mgenerate_and_print_sample\u001b[0;34m(model, tokenizer, device, start_context)\u001b[0m\n\u001b[1;32m      5\u001b[0m encoded \u001b[38;5;241m=\u001b[39m text_to_ids(start_context, tokenizer)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_size\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     decoded_text \u001b[38;5;241m=\u001b[39m ids_to_text(token_ids, tokenizer)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(decoded_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_text_simple() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 256,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 2,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "        }\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    "    )\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2ba43-dddf-4b87-8c35-5a9733dd9a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff363a8b-bfea-48ec-8447-a8862b17af1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

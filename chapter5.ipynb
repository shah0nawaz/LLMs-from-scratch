{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b154435d-bc10-472f-aa0a-a778d868d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2model.gpt import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5da9c5d2-1b5b-437c-bca4-e764565cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 256,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e43d5d45-4d00-4f94-a44c-d5fde08d11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cfg = GPT_CONFIG_124M\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # model= kwargs.get('model')\n",
    "    # idx = kwargs.get('idx')\n",
    "    # max_new_tokens = kwargs.get('max_new_tokens')\n",
    "    # context_size = kwargs.get('context_size')\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "280b808d-4406-473d-af87-28ed191918af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_ids(text, tokenizer):\n",
    "    ids = tokenizer.encode(text, allowed_special={'<endoftext>'})\n",
    "    encoded_batch = torch.tensor(ids).unsqueeze(0)\n",
    "    return encoded_batch\n",
    "\n",
    "def ids_to_text(ids, tokenizer):\n",
    "    flat = torch.tensor(ids).squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0c84d5cd-9ac4-44dc-8afd-fadc4333a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am khan Alchemy guiding \". GOT processed nerd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55881/3930258963.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat = torch.tensor(ids).squeeze(0)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = 'I am khan'\n",
    "\n",
    "generated_ids = generate_text_simple(model=model, \n",
    "                     idx = text_to_ids(text, tokenizer), \n",
    "                     max_new_tokens=6, \n",
    "                    context_size=cfg['context_length'])\n",
    "print(ids_to_text(generated_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "045eac2d-89b8-4542-8576-9945e0850304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'every effort moves'\n",
    "text2 = 'I really like'\n",
    "inputs = torch.tensor([tokenizer.encode(text1), tokenizer.encode(text2)])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "25a60356-2424-41fb-bc2a-c24d92c727f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16833,  3626,  6100],\n",
       "        [27485,   588, 11311]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'every effort moves'\n",
    "text2 = 'really like chocolate'\n",
    "targets = torch.tensor([tokenizer.encode(text1), tokenizer.encode(text2)])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "223a92b6-a48f-4c40-b21e-bbae1c5d6491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "34e33234-df7d-4d43-a4b2-612b72d02ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[23338],\n",
       "         [30577],\n",
       "         [34259]],\n",
       "\n",
       "        [[38245],\n",
       "         [ 5328],\n",
       "         [28657]]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7cb8f4ea-5b36-4321-b198-e224599287a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23338, 30577, 34259])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d78d19a4-84ba-4a81-8e66-9ac9ba5d791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch1: every effort moves\n",
      "outputs batch 1: Ù† strutNAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55881/3930258963.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat = torch.tensor(ids).squeeze(0)\n"
     ]
    }
   ],
   "source": [
    "print(f'Targets batch1: {ids_to_text(targets[0], tokenizer)}')\n",
    "print(f'outputs batch 1: {ids_to_text(token_ids[0].flatten(), tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4eb12227-d427-4cc5-83a8-844849b9f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3189e-05, 2.0514e-05, 1.3648e-05])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "target_probas_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3f32f28e-a7f6-4ba1-b929-21ee7c68d469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7265e-05, 2.6753e-05, 2.7816e-05])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "target_probas_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "15e61190-79de-4d61-9a31-624850f8a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.5096)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_1)))\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bc8b387e-e86d-437c-b1e7-56492bb02149",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten()\n",
    "# print(targets_flat)\n",
    "# logits_flat[,targets_flat]\n",
    "# print(logits_flat.shape)\n",
    "# logg = torch.log(,targets_flat])\n",
    "# logg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5ade847c-ad33-4570-911f-685e99f027d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.6995)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8b9d6458-1b8b-40c6-9eb6-21e57b9627a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity measure for loss is : 44332.41796875\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(f'Perplexity measure for loss is : {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7a5443c6-2ff1-466d-adae-25c11f302335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "981f27cb-18b2-4f56-b722-f9070b36ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total words: 20479\n",
      "The total tokens 5145\n"
     ]
    }
   ],
   "source": [
    "print(f'The total words: {len(text_data)}')\n",
    "print(f'The total tokens {len(tokenizer.encode(text_data))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e73c490b-0e49-4975-8c16-993607d60f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size : 18431 and Validation data size 2048\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio*len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "print(f'Train data size : {len(train_data)} and Validation data size {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6af4f7f8-84ec-46f4-bbe0-48b388f4d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2model.data import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d9c620ac-bf42-41cf-8d75-b45a361ec77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "                    train_data,\n",
    "                    batch_size=2,\n",
    "                    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0\n",
    "                    )\n",
    "val_loader = create_dataloader_v1(\n",
    "                    val_data,\n",
    "                    batch_size=2,\n",
    "                    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    num_workers=0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "01a11c4a-4f03-4652-a755-1fdd7008b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Validation Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print('Train Loader: ')\n",
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "      \n",
    "print('Validation Loader: ')\n",
    "for x,y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1886ed1b-25b9-4651-963a-f8b7eb4cc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the batch loss\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "34a3564c-d963-4294-bd27-8066d93b3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader)==0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    \n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i< num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device)\n",
    "            total_loss +=loss.item()\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ed85ede1-635d-4efe-bf55-554ac435478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (final_norm): LayerNorm()\n",
       "  (final_linear_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (mmhatt): MaskMultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d0744476-8630-41b7-af6f-bee65624503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.992185380723742\n",
      "Validation loss: 10.9991455078125\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model ,device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "    print('Training loss:', train_loss)    \n",
    "    print('Validation loss:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cb77cacf-9e81-472a-8fde-64978bd1f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        \n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "57f3cc00-10d5-4437-964e-f39dfb7e6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model, encoded, 50,context_size\n",
    "        )\n",
    "        decoded_text = ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace('\\n', ' '))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f798c6-ec12-4a2f-94bf-25822167ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fe5df4ae-47bf-445d-bbf6-4fc0d20c40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                      optimizer, device, num_epochs,\n",
    "                      eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen +=input_batch.numel()\n",
    "            global_step+=1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(f\"Ep {epoch+1} (step {global_step:06d}): \"\n",
    "                     f\"Train loss {train_loss:.3f}, \"\n",
    "                     f\"Val_loss {val_loss: .3f}\")\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a34853-2568-405d-872b-00e2a8eeecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (step 000000): Train loss 9.731, Val_loss  9.915\n",
      "Ep 1 (step 000005): Train loss 7.968, Val_loss  8.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55881/3930258963.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat = torch.tensor(ids).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,.                                    \n",
      "Ep 2 (step 000010): Train loss 6.739, Val_loss  7.079\n",
      "Ep 2 (step 000015): Train loss 6.028, Val_loss  6.518\n",
      "Every effort moves you, and the.                                              \n",
      "Ep 3 (step 000020): Train loss 5.354, Val_loss  6.437\n",
      "Ep 3 (step 000025): Train loss 5.286, Val_loss  6.371\n",
      "Every effort moves you of the picture to \" that he had been--I to \" that, and I had beenisburn--and, and to the picture, and that he had been that he had been that he had a to the \" that, and I had\n",
      "Ep 4 (step 000030): Train loss 4.921, Val_loss  6.459\n",
      "Ep 4 (step 000035): Train loss 4.571, Val_loss  6.291\n",
      "Every effort moves you of the picture. \"--. Gisburn--I. Gisburn, and he was a--and. Gisburn, and I had the--and, and I had been. I had been--and, and. The\n",
      "Ep 5 (step 000040): Train loss 4.097, Val_loss  6.446\n",
      "Every effort moves you of a little to see I had been the picture--I was his pictures--as I was not to me--I.                         \n",
      "Ep 6 (step 000045): Train loss 3.717, Val_loss  6.291\n",
      "Ep 6 (step 000050): Train loss 3.164, Val_loss  6.259\n",
      "Every effort moves you know he was not that I felt to the fact--I was his pictures him.               \"I looked at the fact--the I had been the man of the fact in the\n",
      "Ep 7 (step 000055): Train loss 2.701, Val_loss  6.244\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 256,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 2,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "        }\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    "    )\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c7c46-4d4c-426b-90d1-8919fc1bf822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2ba43-dddf-4b87-8c35-5a9733dd9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff363a8b-bfea-48ec-8447-a8862b17af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model,\n",
    "    idx = text_to_ids('Every effort moves you', tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    "    )\n",
    "\n",
    "print(\"Output text: \\n\", ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40014de6-3860-47cb-8a42-b3c83ca1ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "\"closer\": 0,\n",
    "\"every\": 1,\n",
    "\"effort\": 2,\n",
    "\"forward\": 3,\n",
    "\"inches\": 4,\n",
    "\"moves\": 5,\n",
    "\"pizza\": 6,\n",
    "\"toward\": 7,\n",
    "\"you\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe82940-6349-42be-bebe-970f6c96bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da5282-b8d3-450b-a4ce-01990e341caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4847cc-1db5-4737-9eb0-0b2dcad4f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41972b6-f9f2-4f32-b33e-176644af9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_ids = torch.argmax(probas).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2811c-18c3-4852-bbb6-455edd1365fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fae51-7219-4183-97a1-a216dd130578",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab(next_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564a61e-5d37-46e9-bbc1-fb3c7260ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40279ddf-78fd-41e7-b208-eb6975a69de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "        for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5399e80-f77a-4e2a-8b7d-001fe237d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c7371-2cc5-412e-861c-8149301429a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf69869-b2e8-48b5-a087-17ef00aafced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sampled_tokens(logits, temperature):\n",
    "    # torch.manual_seed(123)\n",
    "    scaled_logits = logits / temperature\n",
    "    probas = torch.softmax(scaled_logits, dim=0)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "        for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    print(f'Pizza, ',sampled_ids[vocab['pizza']].item())\n",
    "    # for i, freq in enumerate(sampled_ids):\n",
    "    #     print(f\"{freq} x {inverse_vocab[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e696df-d9dd-4e65-ae49-e3960c7780a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sampled_tokens(next_token_logits, temperature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273f11e-af5d-41ae-ae66-b029e52dcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69636053-13dd-42ce-901b-631580f90c08",
   "metadata": {},
   "source": [
    "## Top K Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6d4f0-7cbe-4604-8bfd-1364f667ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits_new= sorted(next_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4df8c4-8506-4e1b-b0a0-7e92db5767ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits_new[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63ad1c-611a-41ea-b2eb-f9517f81b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48c65a-5f61-4ecb-91ef-7d9e81b17bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits = torch.where(\n",
    "    condition = next_token_logits<next_token_logits_new[-3],\n",
    "    input = torch.tensor(float(\"-inf\")),\n",
    "    other = next_token_logits\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572fb4b8-3158-4729-9cf2-9c299af450e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140507b8-c6cd-45e0-9718-3daf24df1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5fbe8-6bbc-4137-abd7-756844b04571",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641e70b-c870-43bf-9d19-b893034438bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "    temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd24b99-fa08-4d7d-83b7-c9bc003fb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx = text_to_ids('Every effort moves you', tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size = GPT_CONFIG_124M['context_length'],\n",
    "    top_k = 25,\n",
    "    temperature=1.4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b0d29-911a-4928-bd38-774d03c2cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb05707-b47e-4394-8795-9998318d14aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
